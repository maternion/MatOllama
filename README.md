# âš¡ MatOllama â€” Fast, clean terminal workflows for local LLMs
MatOllama is a polished command-line interface for running and managing local LLMs via Ollama. It emphasizes speed, clarity, and ergonomics, with adaptive layouts, responsive streaming chat, and safe controls suited for daily use.

## âœ¨ Features
- Clean terminal UX with rich panels, tables, and spinners
- Streamlined model ops: list, pull, run, show, copy, create, push, remove
- Adaptive table width based on terminal size and model names
- Streaming chat with firstâ€‘token responsiveness
- â€œThinkingâ€ model support: parses â€¦, dims chainâ€‘ofâ€‘thought, shows final answer cleanly
- Session tools: save/load with timestamps and CLI version metadata
- Safety and control: graceful Ctrl+C, stop/unload, confirmations for destructive actions
- Verbose mode (-v/--verbose) for debug fields from the API
- Version panel shows both CLI and Ollama versions

## ğŸ“¦ Install
- Requirements:
  - Python 3.9+
  - Ollama running locally (default: http://localhost:11434)
  - Python packages: requests, rich

Clone and run directly:
```bash
git clone 
cd 
python3 MatOllama.py
```

(Optional) Make executable:
```bash
chmod +x MatOllama.py
./MatOllama.py
```

## ğŸš€ Quick start
1) Start Ollama:
```bash
ollama serve
```

2) Launch MatOllama and list models:
```bash
./MatOllama.py
list # happens by default
```

3) Pull a model:
```bash
pull llama3
```

4) Run a model (by name or number from list):
```bash
run llama3.1
# or
run 2
```

5) Chat mode:
- After `run`, type messages; use `exit` to leave chat mode.
- Use `stop` to halt generation, `unload` to free the model.

## ğŸ§° CLI commands
- list â€” List available models with index, name, digest, size, modified
- pull  â€” Download a model from registry
- run  ["prompt"] [--verbose|-v] â€” Start chat or oneâ€‘shot prompt
- show  â€” Show model metadata
- rm  â€” Delete a local model (with confirmation)
- copy   â€” Copy a model under a new name
- create  [Modelfile] â€” Create from a Modelfile (file or interactive input)
- push  â€” Push to registry (must be logged in)
- ps â€” Show currently running models (with expiry)
- version â€” Show CLI and Ollama versions + recent changes
- unload â€” Unload current model and clear state
- stop â€” Stop current generation
- temp [value] â€” Show/set temperature (0.0â€“2.0)
- system [prompt] â€” Set/clear system prompt
- history â€” Show conversation history
- clear â€” Clear history (with confirm)
- save [file] â€” Save session to ~/.ollama_cli/session_*.json
- load  â€” Load session from file
- help â€” Show help
- exit | quit â€” Exit (prompt to save if history exists)

## ğŸ§ª Examples
- Run by index with immediate prompt:
```bash
run 2 "Write a haiku about monsoons."
```

- Enable verbose debug (extra API fields in dim yellow):
```bash
run llama3 --verbose
```

- Create a model interactively:
```bash
create my-model
# paste Modelfile lines, end with 'END'
```

- Save and load sessions:
```bash
save
load session_20250101_120000.json
```

## ğŸ›ï¸ UX details
- Adaptive width: Calculates optimal table width for model list based on terminal size and longest model name.
- Humanized info: Shows MB/GB sizes and readable timestamps.
- Progress feedback: Detailed status for pull/create/push (manifest, download, verify, cleanup).
- Thinking models: Streams internal content dimmed, then reveals the final answer clearly.
- Signal handling: Ctrl+C gracefully stops active generation; otherwise hints to use `exit`.

## âš™ï¸ Configuration
- Host: `--host` (default http://localhost:11434)
- Timeout: `--timeout` in seconds (default 300)
- Temperature: `temp [0.0â€“2.0]`
- System prompt: `system "You are a helpful assistant."`

## ğŸ†˜ Troubleshooting
- â€œCannot connect to Ollamaâ€
  - Ensure `ollama serve` is running and `--host` is correct.
- â€œModel not found locallyâ€
  - Use `pull` or accept prompt to pull when running by name.
- Terminal layout issues
  - Resize the terminal; the table width recalculates dynamically.
- Stuck generation
  - Use `stop` or Ctrl+C; then `unload` if needed.

## ğŸ™ Acknowledgements
Built for smooth local LLM workflows with Ollama, using requests and rich for a clean terminal experience.
